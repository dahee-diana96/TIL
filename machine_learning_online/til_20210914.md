# Chapter 02. 우리 애는 머리는 좋은데, 공부를 안 해서 그래요

# Step 2. EDA 및 데이터 기초 통계 분석 (2)
- countplot을 보고 알 수 있는 것들은 상관성 분석이지 dependency를 보는 것이 아님!
- `df['Class_value'] = df['Class'].map(dict(L=-1, M=0, H=1))`
    - 분석의 용의성을 높이기 위해 컬럼을 생성함

# Step 3. 모델 학습을 위한 데이터 전처리
- One-hot 벡터란?
    - [출처] 책 '혼자 공부하는 머신러닝+딥러닝'
    - 타깃값을 해당 클래스만 1이고 나머지는 모두 0인 배열로 만드는 벡터
- Multicollinearity (다중공선성)란?
    - [출처: 두산백과] 회귀 분석에서 사용된 모형의 일부 예측 변수가 다른 예측 변수와 상관 정도가 높아, 데이터 분석 시 부정적인 영향을 미치는 현상
- `drop_first`
    - [참고한 링크](https://blog.naver.com/PostView.nhn?blogId=esak97&logNo=221715216490)
    - n-1개의 열을 만들기 위해서 `drop_first=True`를 입력한다.

# Step 4. Classification 모델 학습하기
- 비교한 두 모델: Logistic regression & XGBoost
- from each f1-score's accuracy: 0.69 & 0.72
- 따라서, logistic regression보다 xgb의 성능이 이 데이터에서는 더 좋다는 것을 알 수 있음

# Step 5. 모델 학습 결과 심화 분석하기
- `model_lr.coef_.shape`
    - 결과: (3, 59)
    - 59 = 학습에 쓰고 있는 feature의 개수
    - 3 = class의 개수
- Logistic Regression 모델 계수로 상관성이 높은 요소들과 낮은 요소들을 파악할 수 있음 (by making a graph using plt)
- XGBoost 모델로 특징의 중요도를 요소마다 확인할 수 있음 (by making a graph using plt)

